{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwaZKXoWxMWNOxIsAUQ0Wi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ampehta/MLP-applications-on-Language-Models/blob/main/MLP_LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvvAr4Pn3tqM"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoK-4oXc3qqy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from transformers import AutoTokenizer\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFaZxbKZ6Ikz"
      },
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "embedding_dimension = 768 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxpkKCDCGmqZ"
      },
      "source": [
        "class MLPNet(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,embedding_dimension,block_n):\n",
        "        super(MLPNet,self).__init__()\n",
        "        self.embed_layer = tf.keras.layers.Embedding(vocab_size,embedding_dimension)\n",
        "        self.stacked_MLPBlock = tf.keras.Sequential([MLPBlock() for n in range(block_n)])#tf.keras.Sequential([MLPBlock() for n in range(3)])\n",
        "\n",
        "    def call(self,x):\n",
        "        x = self.embed_layer(x) # (bs,512,768)\n",
        "        x = self.stacked_MLPBlock(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLPBlock(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MLPBlock,self).__init__()\n",
        "        self.conv = tf.keras.Sequential([tf.keras.layers.Conv2D(5,kernel_size=2),\n",
        "                                         tf.keras.layers.Conv2D(1,kernel_size=5)])\n",
        "\n",
        "        self.dense_wise = tf.keras.Sequential([\n",
        "                                               tf.keras.layers.Dense(768*4,activation='relu'),\n",
        "                                               tf.keras.layers.Dense(768/2,activation='relu'),\n",
        "                                               tf.keras.layers.Dense(768,activation='relu'),\n",
        "                                               tf.keras.layers.LayerNormalization()])\n",
        "        self.feature_wise = tf.keras.Sequential([\n",
        "                                                 tf.keras.layers.Dense(512*4,activation='relu'),\n",
        "                                                 tf.keras.layers.Dense(512/2,activation='relu'),       \n",
        "                                                 tf.keras.layers.Dense(512,activation='relu'),\n",
        "                                                 tf.keras.layers.LayerNormalization()])\n",
        "\n",
        "    \n",
        "    def call(self,x): #(bs,512,768)\n",
        "\n",
        "        x1 , x2 ,x3 = tf.split(x,3,2)\n",
        "        x = tf.stack([x1,x2,x3],-1) # cnn 용으로 (bs,512,768) -> (bs,3,512,-1)\n",
        "        x = self.conv(x)\n",
        "        x = tf.squeeze(x)\n",
        "\n",
        "        x = self.dense_wise(x)\n",
        "        x = tf.transpose(x,[0,2,1])\n",
        "\n",
        "        x = self.feature_wise(x)\n",
        "        x = tf.transpose(x,[0,2,1]) # 원상 복귀 \n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_ZDRmDouBJ2"
      },
      "source": [
        "# Benchmark : KLUE TopicClassification dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voWUZV1nlyFK"
      },
      "source": [
        "class MLPBNet_for_tc(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,output_dimension,block_n):\n",
        "        super(MLPBNet_for_tc,self).__init__()\n",
        "        self.MLPNet = MLPNet(vocab_size,output_dimension,block_n)\n",
        "        self.classifier_head = tf.keras.Sequential([\n",
        "                                                tf.keras.layers.LSTM(768,return_sequences=True),\n",
        "                                                tf.keras.layers.LSTM(2048),\n",
        "                                                tf.keras.layers.Dense(512,activation='relu'),\n",
        "                                                tf.keras.layers.Dense(7)])\n",
        "    def call(self,x):\n",
        "        x = self.MLPNet(x)\n",
        "        x = self.classifier_head(x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boZtHkMCUPWE"
      },
      "source": [
        "klue_tc = pd.read_json('https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/ynat-v1/ynat-v1_train.json')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FAUREYOkoWN"
      },
      "source": [
        "X = klue_tc['title'].values\n",
        "y = klue_tc['label'].values\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'])\n",
        "y = le.transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=1210)\n",
        "\n",
        "X_train = X_train.tolist()\n",
        "X_test = X_test.tolist()\n",
        "y_train = y_train.tolist()\n",
        "y_test = y_test.tolist()\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaVGUHZLwm_8"
      },
      "source": [
        "X_train = tokenizer.batch_encode_plus(X_train,padding='max_length',return_tensors='tf')['input_ids']\n",
        "X_test = tokenizer.batch_encode_plus(X_test,padding='max_length',return_tensors='tf')['input_ids']\n",
        "\n",
        "X_train = tf.reshape(tf.constant(X_train),(-1,512))\n",
        "X_test = tf.reshape(tf.constant(X_test),(-1,512))\n",
        "\n",
        "y_train = tf.reshape(tf.constant(y_train),(-1,1))\n",
        "y_test = tf.reshape(tf.constant(y_test),(-1,1))\n",
        "\n",
        "def make_tfdataset(x,y,batch_size):\n",
        "    dataset_x = tf.data.Dataset.from_tensor_slices(x) \n",
        "    dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
        "    dataset = tf.data.Dataset.zip((dataset_x, dataset_y))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = make_tfdataset(X_train,y_train,12)\n",
        "test_dataset = make_tfdataset(X_test,y_test,12)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vzJdE0UxE1U"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "def loss(model, x, y):\n",
        "  y_ = model(x)\n",
        "  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXswyk0inK-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c3d8c0-8676-49a2-e224-a08910cb45fd"
      },
      "source": [
        "TCNet = MLPBNet_for_tc(tokenizer.vocab_size,768,12)\n",
        "\"\"\"\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "TCNet.compile(optimizer=opt, loss=loss)\n",
        "\"\"\"\n",
        "\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 201\n",
        "iter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "  # 훈련 루프 - 32개의 배치를 사용합니다.\n",
        "    for x, y in train_dataset:\n",
        "    # 모델을 최적화합니다.\n",
        "        print(iter)\n",
        "        iter+=1\n",
        "        loss_value, grads = grad(TCNet, x, y)\n",
        "        optimizer.apply_gradients(zip(grads, TCNet.trainable_variables))\n",
        "\n",
        "    # 진행 상황을 추적합니다.\n",
        "        epoch_loss_avg(loss_value)  # 현재 배치 손실을 추가합니다.\n",
        "    # 예측된 레이블과 실제 레이블 비교합니다.\n",
        "        epoch_accuracy(y, TCNet(x))\n",
        "        if iter % 20 == 0:\n",
        "            print(\"에포크 {:03d} iteration:{} 손실: {:.3f}, 정확도: {:.3%}\".format(epoch,iter,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))\n",
        "\n",
        "  # epoch 종료\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "에포크 000 iteration:1780 손실: 1.939, 정확도: 17.893%\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "에포크 000 iteration:1800 손실: 1.939, 정확도: 17.935%\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "에포크 000 iteration:1820 손실: 1.939, 정확도: 17.930%\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "에포크 000 iteration:1840 손실: 1.939, 정확도: 17.899%\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "에포크 000 iteration:1860 손실: 1.939, 정확도: 17.885%\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "에포크 000 iteration:1880 손실: 1.939, 정확도: 17.895%\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "에포크 000 iteration:1900 손실: 1.939, 정확도: 17.934%\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "에포크 000 iteration:1920 손실: 1.939, 정확도: 17.930%\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "에포크 000 iteration:1940 손실: 1.939, 정확도: 17.917%\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "에포크 000 iteration:1960 손실: 1.939, 정확도: 17.900%\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "에포크 000 iteration:1980 손실: 1.939, 정확도: 17.870%\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "에포크 000 iteration:2000 손실: 1.938, 정확도: 17.933%\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "에포크 000 iteration:2020 손실: 1.938, 정확도: 17.950%\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "에포크 000 iteration:2040 손실: 1.938, 정확도: 17.941%\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "에포크 000 iteration:2060 손실: 1.938, 정확도: 17.933%\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "에포크 000 iteration:2080 손실: 1.938, 정확도: 17.993%\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "에포크 000 iteration:2100 손실: 1.938, 정확도: 17.956%\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "에포크 000 iteration:2120 손실: 1.938, 정확도: 17.948%\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "에포크 000 iteration:2140 손실: 1.938, 정확도: 17.971%\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "에포크 000 iteration:2160 손실: 1.938, 정확도: 17.986%\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "에포크 000 iteration:2180 손실: 1.938, 정확도: 17.970%\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "에포크 000 iteration:2200 손실: 1.938, 정확도: 17.962%\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "에포크 000 iteration:2220 손실: 1.938, 정확도: 17.973%\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "에포크 000 iteration:2240 손실: 1.938, 정확도: 17.999%\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "에포크 000 iteration:2260 손실: 1.937, 정확도: 18.013%\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "에포크 000 iteration:2280 손실: 1.937, 정확도: 18.052%\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "에포크 000 iteration:2300 손실: 1.937, 정확도: 18.036%\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "에포크 000 iteration:2320 손실: 1.937, 정확도: 18.028%\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHL9s2w1QJvi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}